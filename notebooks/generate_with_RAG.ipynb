{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04ae239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# === Paths (Windows-safe) ===\n",
    "ADAPTER_DIR = Path(\"../data/lora-Qwen2.5-7B\")      # your saved LoRA adapter\n",
    "CHROMA_DIR  = Path(\"../data/db_chroma\")            # existing Chroma DB directory\n",
    "COLL_NAME   = \"facts\"\n",
    "\n",
    "# === Base model (open, no gating) ===\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 4-bit load for inference if GPU is present\n",
    "bnb_config = None\n",
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765f8d9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      5\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m base = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# None on CPU\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m base = PeftModel.from_pretrained(base, \u001b[38;5;28mstr\u001b[39m(ADAPTER_DIR))\n\u001b[32m     14\u001b[39m base.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yifan\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yifan\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\modeling_utils.py:316\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    318\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yifan\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\modeling_utils.py:5042\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5040\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5041\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5042\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5044\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5045\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yifan\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\modeling_utils.py:1501\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1498\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1504\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yifan\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:117\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "assert ADAPTER_DIR.exists(), f\"Adapter folder not found: {ADAPTER_DIR.resolve()}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16 if device==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device==\"cuda\" else None,\n",
    "    quantization_config=bnb_config,   # None on CPU\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, str(ADAPTER_DIR))\n",
    "base.eval()\n",
    "print(\"Model + LoRA ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1601331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma loaded. Docs: 7\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Must match the embedding model used when building the DB\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "client = PersistentClient(path=str(CHROMA_DIR))\n",
    "rag_coll = client.get_collection(name=COLL_NAME, embedding_function=ef)\n",
    "print(\"Chroma loaded. Docs:\", rag_coll.count())\n",
    "\n",
    "def retrieve_chroma(query: str, k: int = 3):\n",
    "    res = rag_coll.query(query_texts=[query], n_results=k)\n",
    "    docs = res.get(\"documents\", [[]])[0]\n",
    "    metas = res.get(\"metadatas\", [[]])[0]\n",
    "    hits = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        meta = metas[i] if i < len(metas) else {}\n",
    "        if doc:\n",
    "            hits.append({\"text\": doc, \"meta\": meta})\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3acebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ---- (A) Factual detection ----\n",
    "FACTY_KEYWORDS = [\"name\", \"email\", \"phone\", \"birthday\", \"wife\", \"husband\", \"cat\", \"dog\", \"city\", \"university\"]\n",
    "def is_factual_query(q: str) -> bool:\n",
    "    ql = q.lower()\n",
    "    return any(k in ql for k in FACTY_KEYWORDS)\n",
    "\n",
    "# ---- (B) Pinned facts (optional) ----\n",
    "PROFILE_PATH = Path(\"./profile.json\")\n",
    "PROFILE = {}\n",
    "if PROFILE_PATH.exists():\n",
    "    try:\n",
    "        PROFILE = json.loads(PROFILE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        PROFILE = {}\n",
    "\n",
    "def pinned_facts_block(profile: dict) -> str:\n",
    "    if not profile: return \"\"\n",
    "    lines = []\n",
    "    for k, v in profile.items():\n",
    "        if isinstance(v, list):\n",
    "            v = \", \".join(v)\n",
    "        lines.append(f\"- {k}: {v}\")\n",
    "    return \"PINNED FACTS:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "# Canonical strings to preserve exactly (light post-fix)\n",
    "CANONICAL_STRINGS: List[str] = []\n",
    "for key in (\"cat_name\", \"wife_name\", \"husband_name\", \"email\", \"phone\"):\n",
    "    v = PROFILE.get(key)\n",
    "    if isinstance(v, str) and v.strip():\n",
    "        CANONICAL_STRINGS.append(v.strip())\n",
    "\n",
    "def canonicalize(output: str) -> str:\n",
    "    out = output.strip().strip(\"`\")\n",
    "    for canon in CANONICAL_STRINGS:\n",
    "        if not canon: continue\n",
    "        # if exact already present (case-insensitive), leave it\n",
    "        if canon.lower() in out.lower():\n",
    "            continue\n",
    "        base = canon[:3]\n",
    "        if not base: continue\n",
    "        # squash silly repetitions like \"TuaTuaTua\" -> \"Tuantuan\"\n",
    "        out = re.sub(re.escape(base) + r\"{2,}\", canon, out, flags=re.IGNORECASE)\n",
    "    return out\n",
    "\n",
    "# ---- (C) System prompts ----\n",
    "SYS_PROMPT_GENERIC = (\n",
    "    \"You are Mimic‑Me Bot. Be concise, pragmatic, and use 'I' for the user's experience. \"\n",
    "    \"Use information from PINNED FACTS and CONTEXT only; if uncertain, say you're unsure briefly.\"\n",
    ")\n",
    "SYS_PROMPT_FACT = (\n",
    "    \"You are Mimic‑Me Bot. Answer with ONLY the exact value requested, no extra words or punctuation. \"\n",
    "    \"Use information from PINNED FACTS and CONTEXT only; if uncertain, answer: I'm not sure.\"\n",
    ")\n",
    "\n",
    "def build_messages(instruction: str, user_context: str = \"\", retrieved: Optional[List[Dict]] = None, factual: bool = False):\n",
    "    ctx_lines = [h[\"text\"] for h in (retrieved or []) if h.get(\"text\")]\n",
    "    ctx_block = \"\"\n",
    "    if ctx_lines:\n",
    "        ctx_block = \"\\n\\nCONTEXT:\\n\" + \"\\n\".join(f\"- {t}\" for t in ctx_lines)\n",
    "\n",
    "    pin_block = pinned_facts_block(PROFILE)\n",
    "    extra = \"\"\n",
    "    if pin_block: extra += \"\\n\\n\" + pin_block\n",
    "    if ctx_block: extra += ctx_block\n",
    "\n",
    "    system_content = (SYS_PROMPT_FACT if factual else SYS_PROMPT_GENERIC) + (extra or \"\")\n",
    "    user_content = instruction\n",
    "    if factual:\n",
    "        user_content += \"\\n\\nFormat: output only the exact answer, no extra words.\"\n",
    "    elif user_context:\n",
    "        user_content += f\"\\n\\nAdditional details:\\n{user_context}\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\",   \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "# ---- (D) Get special token IDs for clean stopping ----\n",
    "# Qwen uses ChatML: <|im_start|>role ... <|im_end|>\n",
    "IM_END_ID = tokenizer.convert_tokens_to_ids(\"<|im_end|>\") if \"<|im_end|>\" in tokenizer.get_vocab() else tokenizer.eos_token_id\n",
    "\n",
    "@torch.inference_mode()\n",
    "def mebot_generate(\n",
    "    instruction: str,\n",
    "    user_context: str = \"\",\n",
    "    k: int = 3,\n",
    "    use_rag: bool = True,\n",
    "    max_new_tokens: int = 220,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    stream: bool = True\n",
    "):\n",
    "    factual = is_factual_query(instruction)\n",
    "    retrieved = retrieve_chroma(instruction, k=k) if use_rag else []\n",
    "    messages = build_messages(instruction, user_context, retrieved, factual=factual)\n",
    "\n",
    "    # Build prompt using model's chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(base.device) for k, v in inputs.items()}\n",
    "    input_len = int(inputs[\"input_ids\"].shape[1])\n",
    "\n",
    "    if factual:\n",
    "        # Deterministic decode; small token budget; stop on im_end only.\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=min(16, max_new_tokens),\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            eos_token_id=IM_END_ID,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "        out = base.generate(**inputs, **gen_kwargs)\n",
    "        gen_ids = out[0][input_len:]  # <-- decode ONLY the newly generated tokens\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        # Trim at first newline just in case\n",
    "        text = text.split(\"\\n\", 1)[0].strip()\n",
    "        return canonicalize(text)\n",
    "\n",
    "    # Non-factual: allow sampling; support streaming cleanly\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True, temperature=temperature, top_p=top_p,\n",
    "        eos_token_id=IM_END_ID,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        # Stream only the generated part\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        base.generate(**inputs, streamer=streamer, **gen_kwargs)\n",
    "        print()\n",
    "        return \"\"\n",
    "    else:\n",
    "        out = base.generate(**inputs, **gen_kwargs)\n",
    "        gen_ids = out[0][input_len:]  # <-- decode ONLY new tokens\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d53bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PhD is about optimizing database management systems and leveraging AI techniques for performance tuning.\n",
      "\n",
      "MicroTune is an RL (Reinforcement Learning)-based dynamic RAM allocator for MariaDb. Here’s why it is useful:\n",
      "\n",
      "- It optimizes memory usage dynamically to improve query performance.\n",
      "- It reduces the need for manual tuning by automatically adjusting RAM allocation.\n",
      "-它提高了查询性能并减少了手动调优的需要，因此非常有用。请注意，我需要将其翻译回英文。 It improves query performance and reduces the necessity for manual fine-tuning, making it very useful.\n",
      "\n",
      "I speak Chinese, English, and French.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mebot_generate(\"In one sentence, what is my PhD about?\")\n",
    "mebot_generate(\"What is MicroTune and why is it useful? 3 bullets.\", use_rag=True)\n",
    "mebot_generate(\"List the languages you speak.\", use_rag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e46ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tuantuan'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mebot_generate(\"what's my cat's name\", use_rag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
