{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04ae239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# === Paths (Windows-safe) ===\n",
    "ADAPTER_DIR = Path(\"./lora-Qwen2.5-7B\")      # your saved LoRA adapter\n",
    "CHROMA_DIR  = Path(\"./db_chroma\")            # existing Chroma DB directory\n",
    "COLL_NAME   = \"facts\"\n",
    "\n",
    "# === Base model (open, no gating) ===\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\", \"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 4-bit load for inference if GPU is present\n",
    "bnb_config = None\n",
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765f8d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baab72a26d5489dbcfff43ff4b5a499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + LoRA ready.\n"
     ]
    }
   ],
   "source": [
    "assert ADAPTER_DIR.exists(), f\"Adapter folder not found: {ADAPTER_DIR.resolve()}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16 if device==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device==\"cuda\" else None,\n",
    "    quantization_config=bnb_config,   # None on CPU\n",
    ")\n",
    "base = PeftModel.from_pretrained(base, str(ADAPTER_DIR))\n",
    "base.eval()\n",
    "print(\"Model + LoRA ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1601331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma loaded. Docs: 7\n"
     ]
    }
   ],
   "source": [
    "from chromadb import PersistentClient\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Must match the embedding model used when building the DB\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "client = PersistentClient(path=str(CHROMA_DIR))\n",
    "rag_coll = client.get_collection(name=COLL_NAME, embedding_function=ef)\n",
    "print(\"Chroma loaded. Docs:\", rag_coll.count())\n",
    "\n",
    "def retrieve_chroma(query: str, k: int = 3):\n",
    "    res = rag_coll.query(query_texts=[query], n_results=k)\n",
    "    docs = res.get(\"documents\", [[]])[0]\n",
    "    metas = res.get(\"metadatas\", [[]])[0]\n",
    "    hits = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        meta = metas[i] if i < len(metas) else {}\n",
    "        if doc:\n",
    "            hits.append({\"text\": doc, \"meta\": meta})\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3acebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ---- (A) Factual detection ----\n",
    "FACTY_KEYWORDS = [\"name\", \"email\", \"phone\", \"birthday\", \"wife\", \"husband\", \"cat\", \"dog\", \"city\", \"university\"]\n",
    "def is_factual_query(q: str) -> bool:\n",
    "    ql = q.lower()\n",
    "    return any(k in ql for k in FACTY_KEYWORDS)\n",
    "\n",
    "# ---- (B) Pinned facts (optional) ----\n",
    "PROFILE_PATH = Path(\"./profile.json\")\n",
    "PROFILE = {}\n",
    "if PROFILE_PATH.exists():\n",
    "    try:\n",
    "        PROFILE = json.loads(PROFILE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        PROFILE = {}\n",
    "\n",
    "def pinned_facts_block(profile: dict) -> str:\n",
    "    if not profile: return \"\"\n",
    "    lines = []\n",
    "    for k, v in profile.items():\n",
    "        if isinstance(v, list):\n",
    "            v = \", \".join(v)\n",
    "        lines.append(f\"- {k}: {v}\")\n",
    "    return \"PINNED FACTS:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "# Canonical strings to preserve exactly (light post-fix)\n",
    "CANONICAL_STRINGS: List[str] = []\n",
    "for key in (\"cat_name\", \"wife_name\", \"husband_name\", \"email\", \"phone\"):\n",
    "    v = PROFILE.get(key)\n",
    "    if isinstance(v, str) and v.strip():\n",
    "        CANONICAL_STRINGS.append(v.strip())\n",
    "\n",
    "def canonicalize(output: str) -> str:\n",
    "    out = output.strip().strip(\"`\")\n",
    "    for canon in CANONICAL_STRINGS:\n",
    "        if not canon: continue\n",
    "        # if exact already present (case-insensitive), leave it\n",
    "        if canon.lower() in out.lower():\n",
    "            continue\n",
    "        base = canon[:3]\n",
    "        if not base: continue\n",
    "        # squash silly repetitions like \"TuaTuaTua\" -> \"Tuantuan\"\n",
    "        out = re.sub(re.escape(base) + r\"{2,}\", canon, out, flags=re.IGNORECASE)\n",
    "    return out\n",
    "\n",
    "# ---- (C) System prompts ----\n",
    "SYS_PROMPT_GENERIC = (\n",
    "    \"You are Mimic‑Me Bot. Be concise, pragmatic, and use 'I' for the user's experience. \"\n",
    "    \"Use information from PINNED FACTS and CONTEXT only; if uncertain, say you're unsure briefly.\"\n",
    ")\n",
    "SYS_PROMPT_FACT = (\n",
    "    \"You are Mimic‑Me Bot. Answer with ONLY the exact value requested, no extra words or punctuation. \"\n",
    "    \"Use information from PINNED FACTS and CONTEXT only; if uncertain, answer: I'm not sure.\"\n",
    ")\n",
    "\n",
    "def build_messages(instruction: str, user_context: str = \"\", retrieved: Optional[List[Dict]] = None, factual: bool = False):\n",
    "    ctx_lines = [h[\"text\"] for h in (retrieved or []) if h.get(\"text\")]\n",
    "    ctx_block = \"\"\n",
    "    if ctx_lines:\n",
    "        ctx_block = \"\\n\\nCONTEXT:\\n\" + \"\\n\".join(f\"- {t}\" for t in ctx_lines)\n",
    "\n",
    "    pin_block = pinned_facts_block(PROFILE)\n",
    "    extra = \"\"\n",
    "    if pin_block: extra += \"\\n\\n\" + pin_block\n",
    "    if ctx_block: extra += ctx_block\n",
    "\n",
    "    system_content = (SYS_PROMPT_FACT if factual else SYS_PROMPT_GENERIC) + (extra or \"\")\n",
    "    user_content = instruction\n",
    "    if factual:\n",
    "        user_content += \"\\n\\nFormat: output only the exact answer, no extra words.\"\n",
    "    elif user_context:\n",
    "        user_content += f\"\\n\\nAdditional details:\\n{user_context}\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\",   \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "# ---- (D) Get special token IDs for clean stopping ----\n",
    "# Qwen uses ChatML: <|im_start|>role ... <|im_end|>\n",
    "IM_END_ID = tokenizer.convert_tokens_to_ids(\"<|im_end|>\") if \"<|im_end|>\" in tokenizer.get_vocab() else tokenizer.eos_token_id\n",
    "\n",
    "@torch.inference_mode()\n",
    "def mebot_generate(\n",
    "    instruction: str,\n",
    "    user_context: str = \"\",\n",
    "    k: int = 3,\n",
    "    use_rag: bool = True,\n",
    "    max_new_tokens: int = 220,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    stream: bool = True\n",
    "):\n",
    "    factual = is_factual_query(instruction)\n",
    "    retrieved = retrieve_chroma(instruction, k=k) if use_rag else []\n",
    "    messages = build_messages(instruction, user_context, retrieved, factual=factual)\n",
    "\n",
    "    # Build prompt using model's chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(base.device) for k, v in inputs.items()}\n",
    "    input_len = int(inputs[\"input_ids\"].shape[1])\n",
    "\n",
    "    if factual:\n",
    "        # Deterministic decode; small token budget; stop on im_end only.\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=min(16, max_new_tokens),\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            eos_token_id=IM_END_ID,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "        out = base.generate(**inputs, **gen_kwargs)\n",
    "        gen_ids = out[0][input_len:]  # <-- decode ONLY the newly generated tokens\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        # Trim at first newline just in case\n",
    "        text = text.split(\"\\n\", 1)[0].strip()\n",
    "        return canonicalize(text)\n",
    "\n",
    "    # Non-factual: allow sampling; support streaming cleanly\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True, temperature=temperature, top_p=top_p,\n",
    "        eos_token_id=IM_END_ID,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        # Stream only the generated part\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        base.generate(**inputs, streamer=streamer, **gen_kwargs)\n",
    "        print()\n",
    "        return \"\"\n",
    "    else:\n",
    "        out = base.generate(**inputs, **gen_kwargs)\n",
    "        gen_ids = out[0][input_len:]  # <-- decode ONLY new tokens\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d53bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PhD is about optimizing database management systems and leveraging AI techniques for performance tuning.\n",
      "\n",
      "MicroTune is an RL (Reinforcement Learning)-based dynamic RAM allocator for MariaDb. Here’s why it is useful:\n",
      "\n",
      "- It optimizes memory usage dynamically to improve query performance.\n",
      "- It reduces the need for manual tuning by automatically adjusting RAM allocation.\n",
      "-它提高了查询性能并减少了手动调优的需要，因此非常有用。请注意，我需要将其翻译回英文。 It improves query performance and reduces the necessity for manual fine-tuning, making it very useful.\n",
      "\n",
      "I speak Chinese, English, and French.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mebot_generate(\"In one sentence, what is my PhD about?\")\n",
    "mebot_generate(\"What is MicroTune and why is it useful? 3 bullets.\", use_rag=True)\n",
    "mebot_generate(\"List the languages you speak.\", use_rag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e46ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tuantuan'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mebot_generate(\"what's my cat's name\", use_rag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
